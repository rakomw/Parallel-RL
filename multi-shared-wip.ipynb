{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alive-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing  import Process, Queue\n",
    "from queue            import Empty\n",
    "from collections      import deque\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brown-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.weight_backup      = \"cartpole_weight.h5\"\n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=2000)\n",
    "        self.learning_rate      = 0.001\n",
    "        self.gamma              = 0.95\n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01\n",
    "        self.exploration_decay  = 0.995\n",
    "        self.brain              = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            model.load_weights(self.weight_backup)\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        return model\n",
    "\n",
    "    def save_model(self):\n",
    "            self.brain.save(self.weight_backup)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.brain.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        globalMemory.put((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, sample_batch_size):\n",
    "        if globalMemory.qsize() < sample_batch_size:\n",
    "            return\n",
    "        batch_indexes = set(random.sample(range(globalMemory.qsize()), sample_batch_size))\n",
    "        replayCopy = []\n",
    "        while True:\n",
    "            try:\n",
    "                item = globalMemory.get(block=False)\n",
    "            except Empty:\n",
    "                break\n",
    "            else:\n",
    "                replayCopy.append(item)\n",
    "        for item in replayCopy:\n",
    "            globalMemory.put(item)\n",
    "        sample_batch = [replayCopy[index] for index in batch_indexes]\n",
    "        for state, action, reward, next_state, done in sample_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "              target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
    "            target_f = self.brain.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.brain.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "congressional-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake environment specifiers \n",
    "#OBSERVATION_SPEC = tf.TensorSpec([10, 10], tf.uint8)\n",
    "#ACTION_SPEC = tf.TensorSpec([2], tf.float32)\n",
    "#EPISODE_LENGTH = 5\n",
    "#NUM_EPISODES = 10\n",
    "\n",
    "#def agent_step(unused_timestep, pid) -> tf.Tensor:\n",
    "  #return tf.convert_to_tensor([1,9],dtype=float)\n",
    "\n",
    "#def environment_step(unused_action) -> tf.Tensor:\n",
    "  #return tf.cast(tf.random.uniform(OBSERVATION_SPEC.shape, maxval=256),\n",
    "                 #OBSERVATION_SPEC.dtype)\n",
    "class GymEnv:\n",
    "    def __init__(self):\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes          = 10\n",
    "        self.max_ep_length     = 10\n",
    "        self.env               = gym.make('CartPole-v1')\n",
    "\n",
    "        self.state_size        = self.env.observation_space.shape[0]\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        self.agent             = Agent(self.state_size, self.action_size)\n",
    "\n",
    "        \n",
    "        \n",
    "    def actor_thread(self):\n",
    "        pid = os.getpid()\n",
    "        print('actor process:{}\\n'.format(pid))\n",
    "\n",
    "        client = reverb.Client(f'localhost:8000')\n",
    "        with client.trajectory_writer(num_keep_alive_refs=self.max_ep_length+1) as writer:\n",
    "          for _ in range(self.episodes):\n",
    "            timestep = self.env.reset()\n",
    "            #timestep = np.reshape(timestep,[1,self.state_size])\n",
    "            \n",
    "            done = False\n",
    "            i = 0\n",
    "            while (not done and (i < self.max_ep_length)):\n",
    "                action = self.agent.act(timestep)\n",
    "                \n",
    "                newstate, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                #action = np.reshape(action,[1,self.action_size])\n",
    "                \n",
    "                \n",
    "                writer.append({'observation': timestep, 'action': action, \n",
    "                              'reward': reward, 'newstate':newstate})\n",
    "                timestep = newstate\n",
    "                i+=1\n",
    "                \n",
    "            writer.create_item(\n",
    "                table='my_table',\n",
    "                priority=1.5,\n",
    "                trajectory={\n",
    "                    'observations': writer.history['observation'][:],\n",
    "                    'actions': writer.history['action'][:-1],\n",
    "                    'newstates': writer.history['newstate'][:],\n",
    "                    'rewards': writer.history['reward'][:]\n",
    "                }\n",
    "            )\n",
    "            writer.end_episode(timeout_ms=1000)\n",
    "                    \n",
    "        print('actor {} done\\n'.format(pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "delayed-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_server(): \n",
    "    return reverb.Server(\n",
    "    tables=[\n",
    "        reverb.Table(\n",
    "            name='my_table',\n",
    "            sampler=reverb.selectors.Prioritized(priority_exponent=0.8),\n",
    "            remover=reverb.selectors.Fifo(),\n",
    "            max_size=int(1e6),\n",
    "            # Sets Rate Limiter to a low number for the examples.\n",
    "            # Read the Rate Limiters section for usage info.\n",
    "            rate_limiter=reverb.rate_limiters.MinSize(2),\n",
    "            # The signature is optional but it is good practice to set it as it\n",
    "            # enables data validation and easier dataset construction. Note that\n",
    "            # the number of observations is larger than the number of actions.\n",
    "            # The extra observation is the terminal state where no action is\n",
    "            # taken.\n",
    "           # signature={\n",
    "           #     'actions': tf.TensorSpec(\n",
    "           #         [EPISODE_LENGTH, *ACTION_SPEC.shape],\n",
    "           #         ACTION_SPEC.dtype),\n",
    "            #    'observations': tf.TensorSpec(\n",
    "           #         [EPISODE_LENGTH + 1, *OBSERVATION_SPEC.shape],\n",
    "           #         OBSERVATION_SPEC.dtype),\n",
    "          #  },\n",
    "        ),\n",
    "    ],\n",
    "    # Sets the port to None to make the server pick one automatically.\n",
    "    port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "global-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(num_samples):\n",
    "    # Each sample is an entire episode.\n",
    "    # Adjusts the expected shapes to account for the whole episode length.\n",
    "    dataset = reverb.TrajectoryDataset.from_table_signature(\n",
    "      server_address=f'localhost:8000',\n",
    "      table='my_table',\n",
    "      max_in_flight_samples_per_worker=10,\n",
    "      rate_limiter_timeout_ms=10)\n",
    "\n",
    "    # Batches episodes together.\n",
    "    # Each item is an episode of the format (observations, actions) as above.\n",
    "    dataset = dataset.batch(5)\n",
    "\n",
    "    # Sample has type reverb.ReplaySample.\n",
    "    for sample in dataset.take(num_samples):\n",
    "      #print(sample.data['observations'])\n",
    "      print(sample.data['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "selective-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = 10\n",
    "SLEEP_WAIT = 1\n",
    "\n",
    "def server_thread():\n",
    "    pid = os.getpid()\n",
    "    print('server process:{}\\n'.format(pid))\n",
    "    start_time = time.perf_counter()\n",
    "    curr_time = time.perf_counter()\n",
    "    server = start_server()\n",
    "    while((curr_time - start_time) < MAX_TIME):\n",
    "        curr_time = time.perf_counter()\n",
    "        time.sleep(SLEEP_WAIT)\n",
    "    server.stop()\n",
    "    print('server stopped')\n",
    "    display_samples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server process:4537\n",
      "\n",
      "actor process:4540\n",
      "\n",
      "actor process:4545\n",
      "\n",
      "actor process:4553\n",
      "\n",
      "actor process:4561\n",
      "\n",
      "actor process:4569\n",
      "\n",
      "actor 4540 done\n",
      "\n",
      "actor 4569 done\n",
      "\n",
      "actor 4553 done\n",
      "actor 4545 done\n",
      "\n",
      "\n",
      "actor 4561 done\n",
      "\n",
      "server stopped\n"
     ]
    }
   ],
   "source": [
    "num_actors = 5\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    cartpole = GymEnv()\n",
    "    replays = Process(target=server_thread)\n",
    "    replays.start()\n",
    "    \n",
    "    actors = []\n",
    "    for i in range(0,num_actors):\n",
    "        actors.append(Process(target=cartpole.actor_thread))\n",
    "        actors[i].start()\n",
    "        \n",
    "    for actor in actors:\n",
    "        actor.join()\n",
    "    replays.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-mustang",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
