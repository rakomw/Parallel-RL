{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CexLSAF-yHK3"
      },
      "source": [
        "import sys\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from time import sleep\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from multiprocessing import Process, Queue"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ooNRxXU1_wt"
      },
      "source": [
        "class DQNAgent:\r\n",
        "    def __init__(self, state_size, action_size):\r\n",
        "        self.render = False\r\n",
        "        self.weight_backup      = \"cartpole_weight.h5\"\r\n",
        "        # Defining the size of states and actions\r\n",
        "        self.state_size = state_size\r\n",
        "        self.action_size = action_size\r\n",
        "\r\n",
        "        # DQN Hyper parameter\r\n",
        "        self.discount_factor = 0.99\r\n",
        "        self.learning_rate = 0.001\r\n",
        "        self.epsilon = 1.0\r\n",
        "        self.epsilon_decay = 0.9995\r\n",
        "        self.batch_size = 64\r\n",
        "\r\n",
        "        # Create model and target model\r\n",
        "        self.model = self.build_model()\r\n",
        "        self.target_model = self.build_model()\r\n",
        "\r\n",
        "        # Initialize the target model\r\n",
        "        self.update_target_model()\r\n",
        "\r\n",
        "    # build model\r\n",
        "    def build_model(self):\r\n",
        "        model = Sequential()\r\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\r\n",
        "        model.add(Dense(24, activation='relu'))\r\n",
        "        model.add(Dense(self.action_size, activation='linear'))\r\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\r\n",
        "\r\n",
        "        if os.path.isfile(self.weight_backup):\r\n",
        "            model.load_weights(self.weight_backup)\r\n",
        "            self.exploration_rate = self.exploration_min\r\n",
        "        return model\r\n",
        "\r\n",
        "    # Update target model with model's weight\r\n",
        "    def update_target_model(self):\r\n",
        "        self.target_model.set_weights(self.model.get_weights())\r\n",
        "\r\n",
        "    # Choosing behavior with Epsilon greed policy\r\n",
        "    def get_action(self, state):\r\n",
        "        self.epsilon *= self.epsilon_decay\r\n",
        "        if np.random.rand() <= self.epsilon:\r\n",
        "            return random.randrange(self.action_size)\r\n",
        "        else:\r\n",
        "            q_value = self.model.predict(state)\r\n",
        "            return np.argmax(q_value[0])\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO6QWBkJyOOG"
      },
      "source": [
        "def actor(memSample, modelq, q3):\n",
        "    print('start process 1')\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # initializing agent\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    scores = []\n",
        "\n",
        "    for e in range(200):\n",
        "        done = False\n",
        "        score = 0\n",
        "        # reset enviroment\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "\n",
        "        while not done:\n",
        "            if agent.render:\n",
        "                env.render()\n",
        "            #get learner's model and set the weight if it exist.\n",
        "            if modelq.qsize() > 0:\n",
        "                while modelq.qsize() > 1:\n",
        "                    modelq.get()\n",
        "                model = modelq.get()\n",
        "                agent.model.set_weights(model)\n",
        "\n",
        "            # Choose an action as it is\n",
        "            action = agent.get_action(state)\n",
        "            # Advance one time step in the environment with the selected action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            # -100 reward at the end of the episode in the middle\n",
        "            reward = reward if not done or score == 499 else -100\n",
        "\n",
        "            # Save sample <s, a, r, s'> to replay memory\n",
        "            memSample.put([state, action, reward, next_state, done])\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                sleep(0.05)\n",
        "                score = score if score == 500 else score + 100\n",
        "                print(\"episode:\", e, \"  score:\", score, \"  epsilon:\", agent.epsilon)\n",
        "                scores.append(score)\n",
        "\n",
        "                # Stop learning if the average score of the previous 10 episodes is greater than 490\n",
        "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
        "                    #agent.model.save_weights(\"./save_model/cartpole_mp.h5\")\n",
        "                    q3.put(True)\n",
        "                    sys.exit()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAdmcYUFya3F"
      },
      "source": [
        "def learner(memory, modelq, q3):\n",
        "    print('start process 2')\n",
        "    replay_memory = deque(maxlen=5000)\n",
        "    agent = DQNAgent(4, 2)\n",
        "    count = 0\n",
        "    #learning from sample memory\n",
        "    while True:\n",
        "        count += 1\n",
        "        #get a sample from memory\n",
        "        while memory.qsize() > 0:\n",
        "            sample = memory.get()\n",
        "            replay_memory.append(sample)\n",
        "        #if actor finishes exit as well\n",
        "        if q3.qsize() > 0:\n",
        "            sys.exit()\n",
        "\n",
        "        if len(replay_memory) > 1000:\n",
        "            mini_batch = random.sample(replay_memory, agent.batch_size)\n",
        "\n",
        "            states = np.zeros((agent.batch_size, agent.state_size))\n",
        "            next_states = np.zeros((agent.batch_size, agent.state_size))\n",
        "            actions, rewards, dones = [], [], []\n",
        "\n",
        "            for i in range(agent.batch_size):\n",
        "                states[i] = mini_batch[i][0]\n",
        "                actions.append(mini_batch[i][1])\n",
        "                rewards.append(mini_batch[i][2])\n",
        "                next_states[i] = mini_batch[i][3]\n",
        "                dones.append(mini_batch[i][4])\n",
        "\n",
        "            # The model's queuing function for the current state\n",
        "            # Target model's queuing function for the next state\n",
        "            target = agent.model.predict(states)\n",
        "            target_val = agent.target_model.predict(next_states)\n",
        "\n",
        "            # Update target using Bellmann's optimal equation\n",
        "            for i in range(agent.batch_size):\n",
        "                if dones[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else:\n",
        "                    target[i][actions[i]] = rewards[i] + agent.discount_factor * (\n",
        "                        np.amax(target_val[i]))\n",
        "\n",
        "            agent.model.fit(states, target, batch_size=agent.batch_size,\n",
        "                            epochs=1, verbose=0)\n",
        "            model = agent.model.get_weights()\n",
        "            modelq.put(model)\n",
        "            #update target model every 100 step\n",
        "            if count >= 100:\n",
        "                print('update target model')\n",
        "                agent.target_model.set_weights(agent.model.get_weights())\n",
        "                count = 0"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMG05vPkyddW",
        "outputId": "f1438d15-0442-4606-9921-13a30796dcf5"
      },
      "source": [
        "memory = Queue()\n",
        "model = Queue()\n",
        "end = Queue()\n",
        "process1 = Process(target=actor, args=(memory, model, end))\n",
        "process2 = Process(target=learner, args=(memory, model, end))\n",
        "process1.start()\n",
        "process2.start()\n",
        "memory.close()\n",
        "model.close()\n",
        "end.close()\n",
        "memory.join_thread()\n",
        "model.join_thread()\n",
        "end.close()\n",
        "process1.join()\n",
        "process2.join()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start process 1\n",
            "start process 2\n",
            "episode: 0   score: 17.0   epsilon: 0.9910381481909833\n",
            "episode: 1   score: 9.0   epsilon: 0.9860940917766235\n",
            "episode: 2   score: 33.0   epsilon: 0.9694680571640535\n",
            "episode: 3   score: 22.0   epsilon: 0.9583802792808146\n",
            "episode: 4   score: 12.0   epsilon: 0.9521694616616301\n",
            "episode: 5   score: 17.0   epsilon: 0.9436362600491477\n",
            "episode: 6   score: 9.0   epsilon: 0.938928680514662\n",
            "episode: 7   score: 37.0   epsilon: 0.9212530665171942\n",
            "episode: 8   score: 17.0   epsilon: 0.9129969330564653\n",
            "episode: 9   score: 16.0   epsilon: 0.9052674235521029\n",
            "episode: 10   score: 30.0   epsilon: 0.8913405089533699\n",
            "episode: 11   score: 11.0   epsilon: 0.8860071485337379\n",
            "episode: 12   score: 14.0   epsilon: 0.8793853022912325\n",
            "episode: 13   score: 11.0   epsilon: 0.8741234761790619\n",
            "episode: 14   score: 7.0   epsilon: 0.8706330950236377\n",
            "episode: 15   score: 10.0   epsilon: 0.8658565662672015\n",
            "episode: 16   score: 21.0   epsilon: 0.8563819809727236\n",
            "episode: 17   score: 9.0   epsilon: 0.8521096925306499\n",
            "episode: 18   score: 18.0   epsilon: 0.8440509751352934\n",
            "episode: 19   score: 23.0   epsilon: 0.8339803899654946\n",
            "episode: 20   score: 17.0   epsilon: 0.8265063812989984\n",
            "episode: 21   score: 26.0   epsilon: 0.8154207697995194\n",
            "episode: 22   score: 11.0   epsilon: 0.8105416772245603\n",
            "episode: 23   score: 14.0   epsilon: 0.8044838453339194\n",
            "episode: 24   score: 9.0   epsilon: 0.8004704644938051\n",
            "episode: 25   score: 12.0   epsilon: 0.7952829870675735\n",
            "episode: 26   score: 15.0   epsilon: 0.7889445260811917\n",
            "episode: 27   score: 12.0   epsilon: 0.7838317429103805\n",
            "episode: 28   score: 9.0   epsilon: 0.7799213905557423\n",
            "episode: 29   score: 11.0   epsilon: 0.7752547094916237\n",
            "episode: 30   score: 17.0   epsilon: 0.7683069916709178\n",
            "episode: 31   score: 10.0   epsilon: 0.7640918516073671\n",
            "episode: 32   score: 9.0   epsilon: 0.7602799769313064\n",
            "episode: 33   score: 10.0   epsilon: 0.7561088752427627\n",
            "episode: 34   score: 27.0   epsilon: 0.7455944946167565\n",
            "episode: 35   score: 8.0   epsilon: 0.7422460219185594\n",
            "episode: 36   score: 18.0   epsilon: 0.735226326002761\n",
            "episode: 37   score: 19.0   epsilon: 0.7279088814457478\n",
            "episode: 38   score: 14.0   epsilon: 0.7224686311052574\n",
            "episode: 39   score: 20.0   epsilon: 0.7149205202411658\n",
            "episode: 40   score: 27.0   epsilon: 0.7049789010996835\n",
            "episode: 41   score: 11.0   epsilon: 0.7007606404798272\n",
            "episode: 42   score: 21.0   epsilon: 0.6930926077849172\n",
            "episode: 43   score: 9.0   epsilon: 0.6896349316505328\n",
            "episode: 44   score: 17.0   epsilon: 0.6834545255907595\n",
            "episode: 45   score: 13.0   epsilon: 0.678685861447617\n",
            "episode: 46   score: 10.0   epsilon: 0.6749624071563427\n",
            "episode: 47   score: 10.0   epsilon: 0.6712593807428935\n",
            "episode: 48   score: 19.0   epsilon: 0.6645785763045297\n",
            "episode: 49   score: 11.0   epsilon: 0.660602052137845\n",
            "episode: 50   score: 13.0   epsilon: 0.6559928364534608\n",
            "episode: 51   score: 14.0   epsilon: 0.6510900727383278\n",
            "episode: 52   score: 9.0   epsilon: 0.6478419373801442\n",
            "episode: 53   score: 9.0   epsilon: 0.6446100062059081\n",
            "episode: 54   score: 10.0   epsilon: 0.6410735012775659\n",
            "episode: 55   score: 17.0   epsilon: 0.6353282955604289\n",
            "episode: 56   score: 11.0   epsilon: 0.6315267912520551\n",
            "episode: 57   score: 14.0   epsilon: 0.6268068820316673\n",
            "episode: 58   score: 10.0   epsilon: 0.6233680498601484\n",
            "episode: 59   score: 10.0   epsilon: 0.6199480840524855\n",
            "episode: 60   score: 13.0   epsilon: 0.6156225231141397\n",
            "episode: 61   score: 10.0   epsilon: 0.6122450513621789\n",
            "episode: 62   score: 9.0   epsilon: 0.6091907046865513\n",
            "episode: 63   score: 10.0   epsilon: 0.6058485196309628\n",
            "episode: 64   score: 11.0   epsilon: 0.6022234083716455\n",
            "update target model\n",
            "episode: 65   score: 15.0   epsilon: 0.5974236457197061\n",
            "episode: 66   score: 28.0   epsilon: 0.5888213693681873\n",
            "episode: 67   score: 48.0   epsilon: 0.5745670110080087\n",
            "episode: 68   score: 23.0   epsilon: 0.5677117070150417\n",
            "episode: 69   score: 114.0   epsilon: 0.535981342665124\n",
            "episode: 70   score: 61.0   epsilon: 0.5196167909497387\n",
            "episode: 71   score: 36.0   epsilon: 0.510089893972876\n",
            "episode: 72   score: 36.0   epsilon: 0.5007376675755414\n",
            "episode: 73   score: 35.0   epsilon: 0.49180281067107057\n",
            "episode: 74   score: 32.0   epsilon: 0.4837526481106371\n",
            "episode: 75   score: 77.0   epsilon: 0.46524491469918006\n",
            "update target model\n",
            "episode: 76   score: 144.0   epsilon: 0.43270051358644973\n",
            "episode: 77   score: 67.0   epsilon: 0.4182324302984653\n",
            "episode: 78   score: 58.0   epsilon: 0.4060717848186019\n",
            "episode: 79   score: 60.0   epsilon: 0.3938705595770181\n",
            "episode: 80   score: 58.0   epsilon: 0.3824182668015548\n",
            "episode: 81   score: 53.0   epsilon: 0.37222860556614584\n",
            "episode: 82   score: 28.0   epsilon: 0.36686890252465815\n",
            "update target model\n",
            "episode: 83   score: 77.0   epsilon: 0.3528329858813091\n",
            "episode: 84   score: 49.0   epsilon: 0.34411935695070367\n",
            "episode: 85   score: 52.0   epsilon: 0.3351177417096587\n",
            "episode: 86   score: 81.0   epsilon: 0.3216524724011125\n",
            "episode: 87   score: 52.0   epsilon: 0.3132385551383865\n",
            "episode: 88   score: 19.0   epsilon: 0.3101210038765789\n",
            "episode: 89   score: 10.0   epsilon: 0.30841959612920733\n",
            "update target model\n",
            "episode: 90   score: 68.0   epsilon: 0.29795800479541346\n",
            "episode: 91   score: 57.0   epsilon: 0.2894392124362427\n",
            "episode: 92   score: 60.0   epsilon: 0.28074244216874245\n",
            "episode: 93   score: 82.0   epsilon: 0.2693272800799172\n",
            "episode: 94   score: 111.0   epsilon: 0.25465591728182185\n",
            "update target model\n",
            "episode: 95   score: 41.0   epsilon: 0.24936259404858624\n",
            "episode: 96   score: 99.0   epsilon: 0.23719807084571803\n",
            "episode: 97   score: 60.0   epsilon: 0.23007098840005977\n",
            "episode: 98   score: 81.0   epsilon: 0.22082657238351158\n",
            "episode: 99   score: 53.0   epsilon: 0.2149425753056938\n",
            "update target model\n",
            "episode: 100   score: 64.0   epsilon: 0.20806754720104736\n",
            "episode: 101   score: 299.0   epsilon: 0.17907867969532962\n",
            "update target model\n",
            "episode: 102   score: 57.0   epsilon: 0.1739587162651595\n",
            "episode: 103   score: 145.0   epsilon: 0.16170921482609626\n",
            "episode: 104   score: 72.0   epsilon: 0.1559118251558911\n",
            "episode: 105   score: 46.0   epsilon: 0.1522897181523239\n",
            "update target model\n",
            "episode: 106   score: 60.0   epsilon: 0.14771387412025475\n",
            "episode: 107   score: 75.0   epsilon: 0.1422047067631241\n",
            "episode: 108   score: 61.0   epsilon: 0.13786292078522988\n",
            "episode: 109   score: 49.0   epsilon: 0.1344582324962045\n",
            "episode: 110   score: 42.0   epsilon: 0.13159752805795288\n",
            "update target model\n",
            "episode: 111   score: 88.0   epsilon: 0.1258684238591214\n",
            "episode: 112   score: 73.0   epsilon: 0.12129527390642492\n",
            "episode: 113   score: 83.0   epsilon: 0.11630515105741115\n",
            "episode: 114   score: 72.0   epsilon: 0.11213552917125579\n",
            "update target model\n",
            "episode: 115   score: 44.0   epsilon: 0.10964003544793775\n",
            "episode: 116   score: 55.0   epsilon: 0.10661194847014373\n",
            "episode: 117   score: 43.0   epsilon: 0.10429152373446929\n",
            "episode: 118   score: 48.0   epsilon: 0.10176680430243622\n",
            "episode: 119   score: 60.0   epsilon: 0.09870902056115784\n",
            "episode: 120   score: 54.0   epsilon: 0.09603084660910444\n",
            "update target model\n",
            "episode: 121   score: 46.0   epsilon: 0.09379988047351132\n",
            "episode: 122   score: 46.0   epsilon: 0.09162074362063208\n",
            "episode: 123   score: 39.0   epsilon: 0.08980608216305504\n",
            "episode: 124   score: 52.0   epsilon: 0.08745689784194956\n",
            "episode: 125   score: 117.0   epsilon: 0.08244499312695763\n",
            "update target model\n",
            "episode: 126   score: 81.0   epsilon: 0.07913229464095045\n",
            "episode: 127   score: 71.0   epsilon: 0.07633351269264538\n",
            "episode: 128   score: 50.0   epsilon: 0.07441114190713063\n",
            "episode: 129   score: 85.0   epsilon: 0.07127851387707895\n",
            "update target model\n",
            "episode: 130   score: 102.0   epsilon: 0.06769972071415853\n",
            "episode: 131   score: 42.0   epsilon: 0.06625935601562018\n",
            "episode: 132   score: 58.0   epsilon: 0.06433278007397207\n",
            "episode: 133   score: 72.0   epsilon: 0.06202640442891369\n",
            "update target model\n",
            "episode: 134   score: 54.0   epsilon: 0.06034350351735895\n",
            "episode: 135   score: 37.0   epsilon: 0.05920751896649502\n",
            "episode: 136   score: 75.0   epsilon: 0.05699930303058802\n",
            "episode: 137   score: 54.0   epsilon: 0.055452797475230596\n",
            "episode: 138   score: 47.0   epsilon: 0.054137448807183755\n",
            "update target model\n",
            "episode: 139   score: 95.0   epsilon: 0.051599612198765965\n",
            "episode: 140   score: 70.0   epsilon: 0.04979951669042884\n",
            "episode: 141   score: 41.0   epsilon: 0.04876437507065009\n",
            "update target model\n",
            "episode: 142   score: 294.0   epsilon: 0.04207539550047426\n",
            "update target model\n",
            "episode: 143   score: 385.0   epsilon: 0.0346887308151276\n",
            "update target model\n",
            "episode: 144   score: 46.0   epsilon: 0.03388285034580015\n",
            "episode: 145   score: 222.0   epsilon: 0.030307072847376843\n",
            "update target model\n",
            "episode: 146   score: 141.0   epsilon: 0.0282293821761965\n",
            "update target model\n",
            "episode: 147   score: 221.0   epsilon: 0.025262866679751934\n",
            "update target model\n",
            "episode: 148   score: 338.0   epsilon: 0.021323147255757485\n",
            "episode: 149   score: 58.0   epsilon: 0.020703149341900956\n",
            "episode: 150   score: 95.0   epsilon: 0.019732634264684294\n",
            "update target model\n",
            "episode: 151   score: 73.0   epsilon: 0.01901569277382141\n",
            "episode: 152   score: 127.0   epsilon: 0.01783652940946595\n",
            "update target model\n",
            "update target model\n",
            "episode: 153   score: 500.0   epsilon: 0.013890234615068674\n",
            "update target model\n",
            "episode: 154   score: 476.0   epsilon: 0.010942194846292172\n",
            "update target model\n",
            "episode: 155   score: 113.0   epsilon: 0.010335785022560513\n",
            "episode: 156   score: 142.0   epsilon: 0.009622405322636203\n",
            "update target model\n",
            "update target model\n",
            "episode: 157   score: 354.0   epsilon: 0.008057069277366949\n",
            "update target model\n",
            "episode: 158   score: 381.0   epsilon: 0.006655889562725723\n",
            "update target model\n",
            "episode: 159   score: 380.0   epsilon: 0.005501135196814193\n",
            "update target model\n",
            "episode: 160   score: 75.0   epsilon: 0.005295963714893826\n",
            "update target model\n",
            "episode: 161   score: 462.0   epsilon: 0.004201270892186967\n",
            "update target model\n",
            "update target model\n",
            "episode: 162   score: 500.0   epsilon: 0.003271748501867504\n",
            "update target model\n",
            "episode: 163   score: 164.0   epsilon: 0.0030126012860483644\n",
            "update target model\n",
            "episode: 164   score: 500.0   epsilon: 0.002346069557828983\n",
            "update target model\n",
            "update target model\n",
            "episode: 165   score: 500.0   epsilon: 0.0018270065792183054\n",
            "update target model\n",
            "update target model\n",
            "episode: 166   score: 500.0   epsilon: 0.0014227851980637186\n",
            "update target model\n",
            "episode: 167   score: 195.0   epsilon: 0.001289935025767833\n",
            "episode: 168   score: 174.0   epsilon: 0.0011818369522248844\n",
            "update target model\n",
            "episode: 169   score: 229.0   epsilon: 0.0010534191504960265\n",
            "update target model\n",
            "update target model\n",
            "episode: 170   score: 500.0   epsilon: 0.0008203523685852643\n",
            "update target model\n",
            "update target model\n",
            "episode: 171   score: 500.0   epsilon: 0.0006388511242904296\n",
            "update target model\n",
            "update target model\n",
            "episode: 172   score: 500.0   epsilon: 0.0004975066503568284\n",
            "update target model\n",
            "episode: 173   score: 500.0   epsilon: 0.0003874343454027472\n",
            "update target model\n",
            "update target model\n",
            "episode: 174   score: 500.0   epsilon: 0.0003017153075039193\n",
            "update target model\n",
            "update target model\n",
            "episode: 175   score: 500.0   epsilon: 0.00023496142730339167\n",
            "update target model\n",
            "update target model\n",
            "episode: 176   score: 500.0   epsilon: 0.00018297670336043454\n",
            "update target model\n",
            "episode: 177   score: 411.0   epsilon: 0.00014890482203083885\n",
            "update target model\n",
            "update target model\n",
            "episode: 178   score: 500.0   epsilon: 0.00011595994186098363\n",
            "update target model\n",
            "update target model\n",
            "episode: 179   score: 500.0   epsilon: 9.030404746474808e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}