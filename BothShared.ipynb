{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB2BJfaF1FER",
    "outputId": "3f2e4141-75a8-4aa0-e28b-1cdfaf638061",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install ray\n",
    "!pip install baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgsSAWqHn7NX",
    "outputId": "6609d8c7-ac37-41c3-e4f3-39d6397e8c28"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ray\n",
    "import time\n",
    "\n",
    "from collections      import deque\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnFOfh_WPvr4"
   },
   "outputs": [],
   "source": [
    "class ParameterServer(object):\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.opt = optimizers.Adam(lr=0.01)\n",
    "        self.state_size = state_size \n",
    "        self.action_size = action_size\n",
    "        self.model = ParameterServer.build_model(state_size, action_size)\n",
    "        self.target_model = models.clone_model(self.model)\n",
    "        #self.target_model = models.load_model(\"weights.h5\")\n",
    "    \n",
    "    def build_model(state_size, action_size):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.InputLayer(input_shape=state_size))\n",
    "        model.add(layers.Conv2D(32, 8, strides=4, activation=\"relu\"))\n",
    "        model.add(layers.Conv2D(64, 4, strides=2, activation=\"relu\"))\n",
    "        model.add(layers.Conv2D(64, 3, strides=1, activation=\"relu\"))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation=\"relu\"))\n",
    "        model.add(layers.Dense(action_size, activation=\"linear\"))\n",
    "        model.compile(loss='mse', optimizer=optimizers.Adam(lr=0.001))\n",
    "        return model\n",
    "    \n",
    "    def apply_gradients(self, gradients):\n",
    "        var_list = self.model.trainable_variables\n",
    "        self.opt.apply_gradients(zip(gradients, var_list))\n",
    "    \n",
    "    def apply_gradient(self, gradient):\n",
    "        var_list = self.model.trainable_variables\n",
    "        self.opt.apply_gradients(zip(gradient, var_list))\n",
    "    \n",
    "    def sync_target(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return (self.model.get_weights(), self.target_model.get_weights())\n",
    "        \n",
    "    def save_weights(self, version):\n",
    "        if (version > 0):\n",
    "            self.model.save(\"breakout/model{:.0f}.h5\".format(version))\n",
    "        else:\n",
    "            self.model.save(\"breakout/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jtqx61LPvr6"
   },
   "outputs": [],
   "source": [
    "#handle adding to and sampling from a shared replay memory\n",
    "#one of these is created by the main thread \n",
    "class ReplayMemory(object):\n",
    "    def __init__(self):\n",
    "        #deque is an efficient collection for the necessary operations\n",
    "        self.replays = deque(maxlen=100000)\n",
    "\n",
    "    def add_memory(self, replay):\n",
    "      self.replays.append(replay)\n",
    "\n",
    "    def sample_replays(self,batch_size):\n",
    "        #return a randomly sampled batch from the replay memory\n",
    "        return np.array(random.sample(self.replays, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88DN6otoyeKF"
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Learner(object):\n",
    "    def __init__(self, state_size, action_size, batch_size=32, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size \n",
    "        self.sample_batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.Q_network = ParameterServer.build_model(state_size, action_size)\n",
    "        self.target_network = ParameterServer.build_model(state_size, action_size)\n",
    "    \n",
    "    def sync_weights(self, Q_weights, target_weights):\n",
    "        self.Q_network.set_weights(Q_weights)\n",
    "        self.target_network.set_weights(target_weights)\n",
    "\n",
    "    def learn(self, mini_batch):\n",
    "        states = np.zeros((self.sample_batch_size,) + self.state_size)\n",
    "        next_states = np.zeros((self.sample_batch_size,) + self.state_size)\n",
    "        actions, rewards, dones = [], [], []\n",
    "        for i in range(self.sample_batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "        target =  self.Q_network.predict(states)\n",
    "        future_vals = self.target_network.predict(next_states)\n",
    "        for i in range(self.sample_batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + 0.99 * (\n",
    "                    np.amax(future_vals[i]))\n",
    "\n",
    "        dqn_variable = self.Q_network.trainable_variables\n",
    "        with tf.GradientTape() as tape:  \n",
    "            tape.watch(dqn_variable)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            target_q = self.target_network(tf.convert_to_tensor(next_states, dtype=tf.float32))\n",
    "            next_action = tf.argmax(target_q, axis=1)\n",
    "            target_value = tf.reduce_sum(tf.one_hot(next_action, self.action_size) * target_q, axis=1)\n",
    "            target_value = (1-dones) * self.gamma * target_value + rewards\n",
    "\n",
    "            main_q = self.Q_network(tf.convert_to_tensor(states, dtype=tf.float32))\n",
    "            main_value = tf.reduce_sum(tf.one_hot(actions, self.action_size) * main_q, axis=1)\n",
    "\n",
    "            loss = tf.math.reduce_mean(tf.square(main_value - target_value))\n",
    "        return tape.gradient(loss, dqn_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7ktgWlNPvr7"
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Actor(object):\n",
    "    def __init__(self , env_name, state_size, action_size):\n",
    "        self.env = Agent.make_env(env_name) #env\n",
    "        self.state = self.env.reset()\n",
    "        self.Q_network = ParameterServer.build_model(state_size, action_size)\n",
    "        self.done = False\n",
    "        self.total_reward = 0\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.exploration_rate =  1.0\n",
    "        self.exploration_min = 0.1\n",
    "        self.exploration_decay = 0.999\n",
    "        \n",
    "    def sync_weights(self, Q_weights):\n",
    "        self.Q_network.set_weights(Q_weights)\n",
    "        \n",
    "    def run_step(self):\n",
    "        if self.done:\n",
    "            self.env.close()\n",
    "            self.state = self.env.reset()\n",
    "            self.done = False \n",
    "            self.total_reward = 0\n",
    "\n",
    "        state = np.expand_dims(self.state, 0)\n",
    "        \n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            expected_values = self.Q_network.predict(state)\n",
    "            action = np.argmax(expected_values[0])\n",
    "\n",
    "        next_state, reward, self.done, _ = self.env.step(action)\n",
    "        self.total_reward += reward \n",
    "        self.state = next_state \n",
    "\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "      \n",
    "        return np.array([state, action, reward, next_state, self.done, self.total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VzpC8cYQzNk"
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,env_name,num_actors,num_learners):\n",
    "        self.env = Agent.make_env(env_name)\n",
    "        self.state_size = (84,84,4,)\n",
    "        self.action_size = self.env.action_space.n \n",
    "        \n",
    "        self.parameter_server = ParameterServer(self.state_size, self.action_size)\n",
    "        \n",
    "        self.replay_memory = ReplayMemory()\n",
    "        self.actors = [Actor.remote(env_name, self.state_size, self.action_size) \n",
    "                       for k in range(num_actors)]\n",
    "        self.learners = [Learner.remote(self.state_size, self.action_size) \n",
    "                         for _ in range(num_learners)]\n",
    "        \n",
    "    def make_env(env_name):\n",
    "        return wrap_deepmind(make_atari(env_name), frame_stack=True, scale=True)\n",
    "        \n",
    "    def run(self):\n",
    "        start_time = time.time()\n",
    "        counter = 0\n",
    "        recent_scores = deque(maxlen=100)\n",
    "        \n",
    "        #run a little bit to create initial replays before learning\n",
    "        for _ in range(100):\n",
    "            replays = [actor.run_step.remote() for actor in self.actors]\n",
    "            for r in replays:\n",
    "                vals = ray.get(r)\n",
    "                self.replay_memory.add_memory(np.copy(vals[:5]))\n",
    "                if vals[4]:\n",
    "                    recent_scores.append(np.copy(vals[5]))\n",
    "        \n",
    "        #training loop\n",
    "        for counter in range(1000000):\n",
    "            # start actors and learners\n",
    "            replays = [actor.run_step.remote() for actor in self.actors]\n",
    "            gradients = [learner.learn.remote(self.replay_memory.sample_replays(32)) \n",
    "                         for learner in self.learners]\n",
    "            \n",
    "            # save actor results into replay memory\n",
    "            for r in replays:\n",
    "                vals = ray.get(r)\n",
    "                self.replay_memory.add_memory(np.copy(vals[:5]))\n",
    "                if vals[4]:\n",
    "                    recent_scores.append(np.copy(vals[5]))\n",
    "            \n",
    "            # apply gradients\n",
    "            for g in gradients:\n",
    "                self.parameter_server.apply_gradient(ray.get(g))\n",
    "            \n",
    "            if (counter % 100) == 0:\n",
    "                q_weights, target_weights = self.parameter_server.get_weights()\n",
    "                ray.get([actor.sync_weights.remote(q_weights) \n",
    "                        for actor in self.actors])\n",
    "                ray.get([learner.sync_weights.remote(q_weights, target_weights) \n",
    "                          for learner in self.learners])\n",
    "                \n",
    "            if (counter % 500) == 0:\n",
    "                print(\"Step {}: \\tRunning Average: {:.4f}\\tTime: {:.1f}\".format(\n",
    "                    counter, np.mean(recent_scores), (time.time() - start_time) / 60 ))\n",
    "            \n",
    "                if (counter % 5000) == 0:\n",
    "                  self.parameter_server.sync_target()\n",
    "\n",
    "                  if (counter % 10000) == 0:\n",
    "                    self.parameter_server.save_weights((counter / 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aH8plKeeTpMM",
    "outputId": "535f38b0-d8d3-423d-98bb-90a25723c674",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "agent = Agent(\"BreakoutNoFrameskip-v4\",3,1)\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "wip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
