{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgsSAWqHn7NX"
      },
      "source": [
        "# Adapted from code provided in explanatory form at https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762\r\n",
        "import gym\r\n",
        "import random\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from multiprocessing  import Process, Queue\r\n",
        "from queue            import Empty\r\n",
        "from collections      import deque\r\n",
        "from keras.models     import Sequential\r\n",
        "from keras.layers     import Dense\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "globalMemory = Queue()\r\n",
        "\r\n",
        "class Agent():\r\n",
        "    def __init__(self, state_size, action_size):\r\n",
        "        self.weight_backup      = \"cartpole_weight.h5\"\r\n",
        "        self.state_size         = state_size\r\n",
        "        self.action_size        = action_size\r\n",
        "        self.memory             = deque(maxlen=2000)\r\n",
        "        self.learning_rate      = 0.001\r\n",
        "        self.gamma              = 0.95\r\n",
        "        self.exploration_rate   = 1.0\r\n",
        "        self.exploration_min    = 0.01\r\n",
        "        self.exploration_decay  = 0.995\r\n",
        "        self.brain              = self._build_model()\r\n",
        "\r\n",
        "    def _build_model(self):\r\n",
        "        model = Sequential()\r\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\r\n",
        "        model.add(Dense(24, activation='relu'))\r\n",
        "        model.add(Dense(self.action_size, activation='linear'))\r\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\r\n",
        "\r\n",
        "        if os.path.isfile(self.weight_backup):\r\n",
        "            model.load_weights(self.weight_backup)\r\n",
        "            self.exploration_rate = self.exploration_min\r\n",
        "        return model\r\n",
        "\r\n",
        "    def save_model(self):\r\n",
        "            self.brain.save(self.weight_backup)\r\n",
        "\r\n",
        "    def act(self, state):\r\n",
        "        if np.random.rand() <= self.exploration_rate:\r\n",
        "            return random.randrange(self.action_size)\r\n",
        "        act_values = self.brain.predict(state)\r\n",
        "        return np.argmax(act_values[0])\r\n",
        "\r\n",
        "    def remember(self, state, action, reward, next_state, done):\r\n",
        "        self.memory.append((state, action, reward, next_state, done))\r\n",
        "        globalMemory.put((state, action, reward, next_state, done))\r\n",
        "\r\n",
        "    def replay(self, sample_batch_size):\r\n",
        "        if globalMemory.qsize() < sample_batch_size:\r\n",
        "            return\r\n",
        "        batch_indexes = set(random.sample(range(globalMemory.qsize()), sample_batch_size))\r\n",
        "        replayCopy = []\r\n",
        "        while True:\r\n",
        "            try:\r\n",
        "                item = globalMemory.get(block=True, timeout=1)\r\n",
        "            except Empty:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                replayCopy.append(item)\r\n",
        "        for item in replayCopy:\r\n",
        "            globalMemory.put(item)\r\n",
        "        sample_batch = [replayCopy[index] for index in batch_indexes]\r\n",
        "        for state, action, reward, next_state, done in sample_batch:\r\n",
        "            target = reward\r\n",
        "            if not done:\r\n",
        "              target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\r\n",
        "            target_f = self.brain.predict(state)\r\n",
        "            target_f[0][action] = target\r\n",
        "            self.brain.fit(state, target_f, epochs=1, verbose=0)\r\n",
        "        if self.exploration_rate > self.exploration_min:\r\n",
        "            self.exploration_rate *= self.exploration_decay\r\n",
        "\r\n",
        "class CartPole:\r\n",
        "    def __init__(self):\r\n",
        "        self.sample_batch_size = 32\r\n",
        "        self.episodes          = 10\r\n",
        "        self.env               = gym.make('CartPole-v1')\r\n",
        "\r\n",
        "        self.state_size        = self.env.observation_space.shape[0]\r\n",
        "        self.action_size       = self.env.action_space.n\r\n",
        "        self.agent             = Agent(self.state_size, self.action_size)\r\n",
        "\r\n",
        "\r\n",
        "    def run(self):\r\n",
        "        try:\r\n",
        "            for index_episode in range(self.episodes):\r\n",
        "                state = self.env.reset()\r\n",
        "                state = np.reshape(state, [1, self.state_size])\r\n",
        "\r\n",
        "                done = False\r\n",
        "                index = 0\r\n",
        "                while not done:\r\n",
        "                    # self.env.render()\r\n",
        "\r\n",
        "                    action = self.agent.act(state)\r\n",
        "\r\n",
        "                    next_state, reward, done, _ = self.env.step(action)\r\n",
        "                    next_state = np.reshape(next_state, [1, self.state_size])\r\n",
        "                    self.agent.remember(state, action, reward, next_state, done)\r\n",
        "                    state = next_state\r\n",
        "                    index += 1\r\n",
        "                print(\"Episode {}# Score: {}\".format(index_episode, index + 1))\r\n",
        "                self.agent.replay(self.sample_batch_size)\r\n",
        "        finally:\r\n",
        "            self.agent.save_model()\r\n",
        "\r\n",
        "def runDQN():\r\n",
        "    cartpole = CartPole()\r\n",
        "    cartpole.run()\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    processes = []\r\n",
        "    for n in range(3):\r\n",
        "        p = Process(target=runDQN, args=())\r\n",
        "        p.name = 'process' + str(n)\r\n",
        "        processes.append(p)\r\n",
        "        \r\n",
        "    for p in processes:\r\n",
        "        p.start()\r\n",
        "    \r\n",
        "    for p in processes:\r\n",
        "        p.join()\r\n",
        "        \r\n",
        "    print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
