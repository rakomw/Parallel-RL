{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shared-replay.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIE1AI1JmKUW"
      },
      "source": [
        "!pip install ray\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import os\n",
        "import ray\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "from keras.models     import Sequential\n",
        "from keras.layers     import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ray.init()\n",
        "\n",
        "@ray.remote\n",
        "class FinalNetwork(object):\n",
        "    def __init__(self):\n",
        "        self.x = CartPole([], 0)\n",
        "    \n",
        "    def update_final_network(self, state, target):\n",
        "        self.x.agent.brain.fit(state, target, epochs=1, verbose=0)\n",
        "        \n",
        "    def save_final_network(self):\n",
        "        self.x.agent.brain.save(self.x.agent.weights_file)\n",
        "    \n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, state_size, action_size, shared_replay, final_network_actor):\n",
        "        self.weights_file      = \"final_network.h5\"\n",
        "        self.state_size         = state_size\n",
        "        self.action_size        = action_size\n",
        "        self.shared_replay      = list(shared_replay)\n",
        "        self.learning_rate      = 0.001\n",
        "        self.gamma              = 0.95\n",
        "        self.exploration_rate   = 1.0\n",
        "        self.exploration_min    = 0.01\n",
        "        self.exploration_decay  = 0.995\n",
        "        self.brain              = self._build_model()\n",
        "        self.final_network_actor = final_network_actor\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "        if os.path.isfile(self.weights_file):\n",
        "            model.load_weights(self.weights_file)\n",
        "            self.exploration_rate = self.exploration_min\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.shared_replay.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, sample_batch_size):\n",
        "        if len(self.shared_replay) < sample_batch_size:\n",
        "            return\n",
        "        sample_batch = random.sample(self.shared_replay, sample_batch_size)\n",
        "        for state, action, reward, next_state, done in sample_batch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "              target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
        "            target_f = self.brain.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.brain.fit(state, target_f, epochs=1, verbose=0)\n",
        "            self.final_network_actor.update_final_network.remote(state, target_f)\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "class CartPole:\n",
        "    def __init__(self, l, f):\n",
        "        self.sample_batch_size = 32\n",
        "        self.episodes          = 10\n",
        "        self.env               = gym.make('CartPole-v1')\n",
        "\n",
        "        self.state_size        = self.env.observation_space.shape[0]\n",
        "        self.action_size       = self.env.action_space.n\n",
        "        self.agent             = Agent(self.state_size, self.action_size, l, f)\n",
        "        self.f_n               = f\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        for index_episode in range(self.episodes):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "\n",
        "            done = False\n",
        "            index = 0\n",
        "            while not done:\n",
        "                # self.env.render()\n",
        "\n",
        "                action = self.agent.act(state)\n",
        "\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = np.reshape(next_state, [1, self.state_size])\n",
        "                self.agent.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                index += 1\n",
        "            print(\"Episode {}# Score: {}\".format(index_episode, index + 1))\n",
        "            self.agent.replay(self.sample_batch_size)\n",
        "\n",
        "def worker(l, f):\n",
        "    cartpole = CartPole(l, f)\n",
        "    cartpole.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    manager = mp.Manager()\n",
        "    shared_list = manager.list()\n",
        "    \n",
        "    final_network = FinalNetwork.remote()\n",
        "    \n",
        "    processes = []\n",
        "    \n",
        "    for i in range(3):\n",
        "        processes.append(mp.Process(target=worker, args=(shared_list, final_network)))\n",
        "        \n",
        "    for i in range(3):\n",
        "        processes[i].start()\n",
        "\n",
        "    for i in range(3):\n",
        "        processes[i].join()\n",
        "        \n",
        "    print(\"\\n\\nTraining is finished.\\n\\n\")\n",
        "    \n",
        "    final_network.save_final_network.remote()\n",
        "    \n",
        "    print(\"Neural network saved.\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}